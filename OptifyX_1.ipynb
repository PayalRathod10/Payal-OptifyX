{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1] MOVIE REVIEW SYSTEM\n"
      ],
      "metadata": {
        "id": "ogidZBrLN1yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "Dgs56xYB445O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to hold chunks\n",
        "chunks = []"
      ],
      "metadata": {
        "id": "JFJvzbhg45ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV in chunks with error handling\n",
        "chunk_size = 5000\n",
        "for chunk in pd.read_csv(\"/content/IMDB Dataset.csv\", chunksize=chunk_size, encoding='utf-8', quoting=csv.QUOTE_NONE, on_bad_lines='skip'):\n",
        "    chunks.append(chunk)"
      ],
      "metadata": {
        "id": "Ik3boUfv47Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all chunks into a single DataFrame\n",
        "df = pd.concat(chunks, ignore_index=True)"
      ],
      "metadata": {
        "id": "LCt_Jki-499N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure correct column names\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l017ttGd5Bzt",
        "outputId": "907b0bab-4266-45d8-b5fe-fde150e0877a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['review', 'sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle NaN values in both 'review' and 'sentiment' columns\n",
        "df = df.dropna(subset=['review', 'sentiment'])  # Drop rows where 'review' or 'sentiment' has NaN values"
      ],
      "metadata": {
        "id": "4fsAjFr25ENo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and labels (y)\n",
        "X = df['review']  # The text reviews\n",
        "y = df['sentiment']  # The sentiment (1 or 0)\n"
      ],
      "metadata": {
        "id": "3omWx8jI8o7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)"
      ],
      "metadata": {
        "id": "SYX9rbr68rsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training (70%) and testing (30%) data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "8Sj76nA38u3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)  # Train the model\n",
        "y_pred_nb = nb_model.predict(X_test)  # Predict on the test data\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))  # Print accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGF0lmtB82W8",
        "outputId": "dcdf4a64-dd1e-494a-a20b-9f581d3574f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.5909090909090909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression classifier\n",
        "lr_model = LogisticRegression(max_iter=2000)  # Increase max_iter if needed\n",
        "lr_model.fit(X_train, y_train)  # Train the model\n",
        "y_pred_lr = lr_model.predict(X_test)  # Predict on the test data\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))  # Print accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeqKxfuA85D3",
        "outputId": "bb8c4287-3063-4d52-f249-0f1aa30b0fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.5454545454545454\n"
          ]
        }
      ]
    }
  ]
}